{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGlYmbCam7ip"
   },
   "outputs": [],
   "source": [
    "# @title Pip install comands\n",
    "!pip install librosa\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install torch\n",
    "!pip install jiwer\n",
    "!pip install wandb\n",
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "h-PUB4rEmSNT"
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from jiwer import wer\n",
    "from google.colab import drive\n",
    "import pdb\n",
    "import wandb\n",
    "from datasets import load_dataset, load_metric\n",
    "import random\n",
    "cer_metric = load_metric(\"cer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tdJCXEBhm7sV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3f5ef224-8439-4582-a347-df5e5f0070f0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# @title Mount drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "dir_path = '/content/drive/My Drive/AudioFinalProject/data'\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ql5NwyVTn8s4"
   },
   "outputs": [],
   "source": [
    "# @title Constants\n",
    "\n",
    "WAV_PATH = \"/wav\"\n",
    "TXT_PATH = \"/txt\"\n",
    "TRAIN_PATH = \"/train\"\n",
    "VAL_PATH = \"/val\"\n",
    "TEST_PATH = \"/test\"\n",
    "MODEL_FILE_PATH = dir_path + \"/model_files/model_weights.pth\"\n",
    "\n",
    "hparams = {\n",
    "      \"n_cnn_layers\": 4,\n",
    "      \"n_rnn_layers\": 3,\n",
    "      \"rnn_dim\": 512,\n",
    "      \"n_class\": 28,\n",
    "      \"n_feats\": 16,\n",
    "      \"stride\": 2,\n",
    "      \"dropout\": 0.1,\n",
    "      \"learning_rate\": 0.0005,\n",
    "      \"batch_size\": 16,\n",
    "      \"epochs\": 200\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ACOXYCrPm7wE"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, labels, input_lengths, label_lengths,is_train):\n",
    "        \"\"\"\n",
    "        Init CustomDataset obj\n",
    "        :param data: spectrograms extracted from wav files\n",
    "        :param labels: text of the wav files\n",
    "        :param input_lengths: of wav\n",
    "        :param label_lengths: of text\n",
    "        :param is_train: boolean\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.input_lengths = input_lengths\n",
    "        self.label_lengths = label_lengths\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        returns data[idx]\n",
    "        :param idx: index\n",
    "        :return: data_sample, labels_sample, input_lengths_sample, label_lengths_sample\n",
    "        \"\"\"\n",
    "        # Retrieve individual samples using the provided index\n",
    "        data_sample = self.data[idx]\n",
    "        labels_sample = self.labels[idx]\n",
    "        input_lengths_sample = self.input_lengths[idx]\n",
    "        label_lengths_sample = self.label_lengths[idx]\n",
    "\n",
    "        # You may need to convert your data to PyTorch tensors if they are not already\n",
    "        data_sample = torch.tensor(data_sample, dtype=torch.float32)\n",
    "        labels_sample = torch.tensor(labels_sample, dtype=torch.long)\n",
    "        input_lengths_sample = torch.tensor(input_lengths_sample, dtype=torch.int32)\n",
    "        label_lengths_sample = torch.tensor(label_lengths_sample, dtype=torch.int32)\n",
    "\n",
    "        masking = nn.Sequential(\n",
    "            torchaudio.transforms.FrequencyMasking(freq_mask_param=15),\n",
    "            torchaudio.transforms.TimeMasking(time_mask_param=20))\n",
    "\n",
    "        if self.is_train: # masking  train\n",
    "            augmentation_prob = random.random()\n",
    "            if augmentation_prob < 0.4:\n",
    "                data_sample = masking(data_sample)\n",
    "\n",
    "        return data_sample, labels_sample, input_lengths_sample, label_lengths_sample\n",
    "\n",
    "# base model - phase 1\n",
    "class CNN(torch.nn.Module):\n",
    "\n",
    "  def __init__(self,stride=2):\n",
    "    \"\"\"\n",
    "    Base model - build from 3 convolution layers followed by BatchNorm and Relu, and finaly a fully connected layer.\n",
    "    :param stride: how much the filter moves across the input\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.cnn = nn.Sequential(\n",
    "        nn.Conv2d(1, 32, 3, stride, padding=3 // 2),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, 3, 1, padding=3 // 2),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, 3, 1, padding=3 // 2),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    self.fc = torch.nn.Linear(256, 28)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Takes the input image and propagates it through the network, layer by layer.\n",
    "    :param x: input\n",
    "    :return: output of the cnn\n",
    "    \"\"\"\n",
    "    x = self.cnn(x)\n",
    "    sizes= x.shape\n",
    "    x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "    x = x.transpose(1, 2)\n",
    "    x = self.fc(x)\n",
    "    return x\n",
    "\n",
    "# second model - phase 2\n",
    "class model2(nn.Module):\n",
    "    def __init__(self, stride, n_feats, rnn_dim, n_class, n_layers=1, bidirectional=True, dropout_prob=0.1):\n",
    "        \"\"\"\n",
    "        This model is based on the previous model, but with the following additions:\n",
    "        * Dropout after the ReLU activation function in the CNN layer.\n",
    "        * A fully connected layer between the CNN and LSTM layers.\n",
    "        * An LSTM layer to model sequential data and capture long range dependencies.\n",
    "        :param stride: Stride of the convolution operation in the CNN layer.\n",
    "        :param n_feats: Number of features from the input audio data.\n",
    "        :param rnn_dim: Dimensionality of the hidden state in the LSTM layer.\n",
    "        :param n_class: Number of classes in the classification task.\n",
    "        :param n_layers: Number of layers in the LSTM layer.\n",
    "        :param bidirectional:  Whether to use a bidirectional LSTM layer.\n",
    "        :param dropout_prob: Dropout probability.\n",
    "        \"\"\"\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = round(n_feats / 2)\n",
    "\n",
    "        # CNN layer\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, stride, padding=3 // 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, 1, padding=3 // 2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(dropout_prob)  # Adding dropout after ReLU\n",
    "        )\n",
    "\n",
    "\n",
    "        # FC layer needed before lstm\n",
    "        self.fc = nn.Linear(n_feats * 32, rnn_dim)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.rnn = nn.LSTM(input_size=rnn_dim, hidden_size=rnn_dim, num_layers=n_layers,\n",
    "                           batch_first=True, bidirectional=bidirectional, dropout=dropout_prob)\n",
    "\n",
    "        # Classifier layer\n",
    "        self.classifier = nn.Linear(rnn_dim * 2 if bidirectional else rnn_dim, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Takes the input and propagates it through the network, layer by layer.\n",
    "        :param x: input\n",
    "        :return: output of the cnn\n",
    "        \"\"\"\n",
    "        x = self.cnn(x)\n",
    "        batch, channel, feature, time = x.size()\n",
    "        x = x.view(batch, channel * feature, time)  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2)  # (batch, time, feature)\n",
    "        x = self.fc(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# phase 3 - deep speech model\n",
    "class CNNLayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, n_feats):\n",
    "        \"\"\"\n",
    "        CNN Model\n",
    "        :param n_feats: Number of features from the input audio data.\n",
    "        \"\"\"\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Takes the input and propagates it through the network, layer by layer.\n",
    "        :param x: input\n",
    "        :return: output of the cnn\n",
    "        \"\"\"\n",
    "        x = x.transpose(2, 3).contiguous()  # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous()  # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        \"\"\"\n",
    "        Residual CNNs uses shortcut connections to help the network learn more complex features.\n",
    "        :param in_channels: of input\n",
    "        :param out_channels: of input\n",
    "        :param kernel: size\n",
    "        :param stride: Stride of the convolution operation in the CNN layer.\n",
    "        :param dropout: Dropout probability.\n",
    "        :param n_feats:  Number of features from the input audio data.\n",
    "        \"\"\"\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel // 2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel // 2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Takes the input and propagates it through the network, layer by layer.\n",
    "        :param x: input\n",
    "        :return: output of the cnn\n",
    "        \"\"\"\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x  # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        \"\"\"\n",
    "        GRUs are a type of recurrent neural network (RNN) that are well-suited for modeling sequential data, such as speech and text, because they allow to learn long-range dependencies in the data from both directions.\n",
    "        :param rnn_dim:  Dimensionality of the hidden state in the GRU layer.\n",
    "        :param hidden_size: Dimensionality of the hidden state in the GRU layer.\n",
    "        :param dropout: Dropout probability.\n",
    "        :param batch_first: Whether the input data is in batch-first format.\n",
    "        \"\"\"\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Takes the input and propagates it through the network, layer by layer.\n",
    "        :param x: input\n",
    "        :return: output of the cnn\n",
    "        \"\"\"\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    speech recognition model that uses a combination of convolutional neural networks (CNNs) and bidirectional recurrent neural networks (RNNs) to extract features from audio and after CTC process predicts the text said in the audio file.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats // 2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3 // 2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats * 32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i == 0 else rnn_dim * 2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i == 0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim * 2, rnn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Takes the input and propagates it through the network, layer by layer.\n",
    "        :param x: input\n",
    "        :return: output of the cnn\n",
    "        \"\"\"\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LS1saTbwm7y2"
   },
   "outputs": [],
   "source": [
    "def pad_strings_to_equal_length(str1, str2):\n",
    "    \"\"\"\n",
    "    The function first finds the lengths of the two strings and then determines which string is longer and which string is shorter. It then calculates the difference in lengths and pads the shorter string with spaces to match the length of the longer string\n",
    "    :param str1: text string\n",
    "    :param str2: text string\n",
    "    :return: a list of the two strings, with the shorter string padded with spaces.\n",
    "    \"\"\"\n",
    "    # Find the lengths of the strings\n",
    "    len1 = len(str1)\n",
    "    len2 = len(str2)\n",
    "\n",
    "    # Determine the longer and shorter strings\n",
    "    if len1 > len2:\n",
    "        longer_str = str1\n",
    "        shorter_str = str2\n",
    "        length_diff = abs(len1 - len2)\n",
    "\n",
    "        # Pad the shorter string with spaces\n",
    "        padded_shorter_str = shorter_str + ' ' * length_diff\n",
    "\n",
    "        return [longer_str], [padded_shorter_str]\n",
    "    else:\n",
    "        longer_str = str2\n",
    "        shorter_str = str1\n",
    "\n",
    "    # Calculate the difference in lengths\n",
    "    length_diff = abs(len1 - len2)\n",
    "\n",
    "    # Pad the shorter string with spaces\n",
    "    padded_shorter_str = shorter_str + ' ' * length_diff\n",
    "\n",
    "    return [padded_shorter_str], [longer_str]\n",
    "\n",
    "def map_txt_to_int(text):\n",
    "    \"\"\"\n",
    "    Converts each character to an integer using the dictionary.\n",
    "    :param text: string.\n",
    "    :return: a list of the integers\n",
    "    \"\"\"\n",
    "    char_to_integer = {' ': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7,\n",
    "                       'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14,\n",
    "                       'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21,\n",
    "                       'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26}\n",
    "\n",
    "    # Convert text to a list of integers using the mapping\n",
    "    integer_list = [char_to_integer[char] for char in text]\n",
    "    return integer_list\n",
    "\n",
    "\n",
    "def map_int_to_txt(labels):\n",
    "    \"\"\"\n",
    "    Maps the integer 0 to the space character and the integers 1-26 to the letters A-Z.\n",
    "    :param labels: integers\n",
    "    :return: text\n",
    "    \"\"\"\n",
    "    integer_to_char = {0: ' ', 1: 'A', 2: 'B', 3: 'C', 4: 'D', 5: 'E', 6: 'F', 7: 'G',\n",
    "                       8: 'H', 9: 'I', 10: 'J', 11: 'K', 12: 'L', 13: 'M', 14: 'N',\n",
    "                       15: 'O', 16: 'P', 17: 'Q', 18: 'R', 19: 'S', 20: 'T', 21: 'U',\n",
    "                       22: 'V', 23: 'W', 24: 'X', 25: 'Y', 26: 'Z'}\n",
    "\n",
    "    # Convert list of integers to text using the reverse mapping\n",
    "    text = ''.join(integer_to_char[integer] for integer in labels)\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_data(dir_path,hparams, is_train=True):\n",
    "    \"\"\"\n",
    "    Loads audio and text data for training or testing.\n",
    "    :param dir_path:  The path to the directory containing the audio and text data.\n",
    "    :param hparams: hyperparameters.\n",
    "    :param is_train: Whether the data is being loaded for training or testing.\n",
    "    :return: spectograms, labels, input_lengths, label_lengths.\n",
    "    \"\"\"\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "\n",
    "    wav_files_list = os.listdir(dir_path + WAV_PATH)\n",
    "\n",
    "    # 1.load wav and txt files and find max_length\n",
    "    for wav_filename in wav_files_list:\n",
    "        txt_filename = wav_filename[:-3] + \"txt\"  # replace 'wav' end with 'txt'\n",
    "        wav_path = os.path.join(dir_path + WAV_PATH, wav_filename)\n",
    "        txt_path = os.path.join(dir_path + TXT_PATH, txt_filename)\n",
    "        waveform, sr = librosa.load(wav_path)\n",
    "        waveform = torch.tensor(waveform)\n",
    "\n",
    "        extract_mfcc = torchaudio.transforms.MFCC(\n",
    "          sample_rate=sr,\n",
    "          n_mfcc=hparams[\"n_feats\"],\n",
    "          )\n",
    "        spec = extract_mfcc(waveform).T\n",
    "\n",
    "        spectrograms.append(spec)\n",
    "\n",
    "        # fill labels with txt\n",
    "        with open(txt_path) as f:\n",
    "            txt = f.readlines()[0]\n",
    "        label = torch.Tensor(map_txt_to_int(txt))\n",
    "        labels.append(label)\n",
    "\n",
    "        # fill input_lengths and label_lengths\n",
    "        input_lengths.append(spec.shape[0] // 2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    # apply padding to spectrograms and to labels\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True)\n",
    "    spectrograms = spectrograms.unsqueeze(1).transpose(2, 3)  # (data_size, channel, freq, spec_length)\n",
    "\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "    return spectrograms, labels, torch.from_numpy(np.array(input_lengths)), torch.from_numpy(np.array(label_lengths))\n",
    "\n",
    "\n",
    "def GreedyDecoder(output, labels, label_lengths, blank_label=27, collapse_repeated=True):\n",
    "    \"\"\"\n",
    "    Decodes the output of a speech recognition model using a greedy decoder.\n",
    "    :param output: A tensor of output probabilities.\n",
    "    :param labels: A tensor of ground truth labels.\n",
    "    :param label_lengths: A tensor of label lengths.\n",
    "    :param blank_label: The index of the blank label in the output.\n",
    "    :param collapse_repeated: Whether to collapse repeated characters in the decoded output.\n",
    "    :return: decodes - A list of decoded strings, one for each batch element.\n",
    "             targets: A list of ground truth strings, one for each batch element.\n",
    "    \"\"\"\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(map_int_to_txt(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j - 1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(map_int_to_txt(decode))\n",
    "    return decodes, targets\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    function is used to train a speech recognition model on a training dataset.\n",
    "    :param model: The speech recognition model.\n",
    "    :param device: The device to train the model on - Better use GPU from google colab.\n",
    "    :param train_loader: A data loader for the training data.\n",
    "    :param criterion: A loss function.\n",
    "    :param optimizer: An optimizer.\n",
    "    :param epoch: Number of epochs.\n",
    "    :return: A tuple of two floats: the training WER and the training CER\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    train_wer,train_cer = [],[]\n",
    "\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(spectrograms)  # (batch, time, n_class)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1)  # (time, batch, n_class)\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(spectrograms), data_len,\n",
    "                   100. * batch_idx / len(train_loader), loss.item()))\n",
    "        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "        for j in range(len(decoded_preds)):\n",
    "            train_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "            str1, str2 =  pad_strings_to_equal_length(decoded_targets[j], decoded_preds[j])\n",
    "            train_cer.append(cer_metric.compute(references=str1, predictions=str2))\n",
    "\n",
    "    train_wer = sum(train_wer) / len(train_wer)\n",
    "    train_cer = sum(train_cer) / len(train_cer)\n",
    "    return train_wer, train_cer\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion, is_print=False):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a speech recognition model on a test dataset.\n",
    "    The printing part of the function is responsible for printing the outputs of the model for a random sample of 20 batches.\n",
    "    :param model: The speech recognition model.\n",
    "    :param device: The device to evaluate the model on.\n",
    "    :param test_loader: A data loader for the test data.\n",
    "    :param criterion: A loss function.\n",
    "    :param is_print: Whether to print the outputs of the model.\n",
    "    :return: A tuple of two floats: the training WER and the training CER\n",
    "    \"\"\"\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_wer, test_cer = [], []\n",
    "    print_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1)  # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "\n",
    "            for j in range(len(decoded_preds)):\n",
    "                if is_print:\n",
    "                    output_txt = \"Target: \" + decoded_targets[j] + \"\\n\"\n",
    "                    output_txt += \"Predic: \" + decoded_targets[j] + \"\\n\"\n",
    "                    output_txt += \"-----------------------------------------------\"\n",
    "                    print_outputs.append(output_txt)\n",
    "\n",
    "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "                str1, str2 =  pad_strings_to_equal_length(decoded_targets[j], decoded_preds[j])\n",
    "                test_cer.append(cer_metric.compute(references=str1, predictions=str2))\n",
    "        if is_print:\n",
    "            for output_txt in random.sample(print_outputs, 20):\n",
    "              print(output_txt)\n",
    "\n",
    "        test_wer = sum(test_wer) / len(test_wer)\n",
    "        test_cer = sum(test_cer) / len(test_cer)\n",
    "        return test_wer, test_cer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data_loader(data_path,hparams,is_train):\n",
    "    \"\"\"\n",
    "    Loads a data loader for a speech recognition dataset.\n",
    "    :param data_path: The path to the dataset directory.\n",
    "    :param hparams: A dictionary of hyperparameters.\n",
    "    :param is_train: Whether to load the training data or the test data.\n",
    "    :return: A DataLoader object.\n",
    "    \"\"\"\n",
    "    data = load_data(data_path,hparams, is_train)\n",
    "    dataset = CustomDataset(*data, is_train)\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=hparams[\"batch_size\"], shuffle=True)\n",
    "    return data_loader\n",
    "\n",
    "def save_model(model, model_path):\n",
    "    \"\"\"\n",
    "    Saves the model weights and biases to a file.\n",
    "    :param model: The speech recognition model.\n",
    "    :param model_path: Path to the file where the weights will be saved.\n",
    "    \"\"\"\n",
    "    weights = model.state_dict()\n",
    "    torch.save(weights, model_path)\n",
    "\n",
    "def load_model_weights(model, load_path):\n",
    "    \"\"\"\n",
    "    Load the weights of the PyTorch model from a file.\n",
    "    :param model (torch.nn.Module): The PyTorch model to load the weights into.\n",
    "    :param load_path (str): The path to the file from which the model weights will be loaded.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # pdb.set_trace()\n",
    "        model.load_state_dict(torch.load(load_path), strict=False)\n",
    "        print(\"Model weights loaded successfully from:\", load_path)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: The file\", load_path, \"does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzJSBu46pPlA"
   },
   "outputs": [],
   "source": [
    "# @title Load and process data\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_loader = get_data_loader(dir_path + TRAIN_PATH,hparams, is_train=True)\n",
    "val_loader = get_data_loader(dir_path + VAL_PATH,hparams, is_train=False)\n",
    "test_loader = get_data_loader(dir_path + TEST_PATH,hparams, is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Train and Test (saves WER)\n",
    "wandb.init(\n",
    "    project=\"AudioFinalProject - WER2\",\n",
    "    name=f\"Model AudioFinalProject\",\n",
    "    )\n",
    "# Phase 1 - cnn model\n",
    "# model = CNN(hparams['stride']).to(device) #conv model\n",
    "\n",
    "# Phase 2 - cnn and lstm model\n",
    "# model = model2(hparams['stride'], hparams['n_feats'], hparams['rnn_dim'], hparams['n_class']).to(\n",
    "#     device)\n",
    "\n",
    "# phase 3 - DeepSpeech model\n",
    "model = SpeechRecognitionModel(\n",
    "    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "    hparams['n_class'],  hparams[\"n_feats\"], hparams['stride'], hparams['dropout']\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), hparams['learning_rate'])\n",
    "criterion = nn.CTCLoss(blank=27).to(device)\n",
    "\n",
    "\n",
    "# load_model_weights(model, MODEL_FILE_PATH)\n",
    "\n",
    "for epoch in range(1, hparams['epochs'] + 1):\n",
    "    # train and validate model\n",
    "    train_wer, train_cer  = train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "    val_wer, val_cer = test(model, device, val_loader, criterion) # validation\n",
    "\n",
    "    # log the results\n",
    "    wandb.log({\"train wer\": train_wer,\"train cer\": train_cer,\"val wer\": val_wer,\"val cer\": val_cer})\n",
    "\n",
    "\n",
    "test_wer, test_cer = test(model, device, test_loader, criterion)\n",
    "print(\"test wer: \" + str(test_wer))\n",
    "print(\"test cer: \" + str(test_cer))\n",
    "\n",
    "# save_model(model, MODEL_FILE_PATH)\n"
   ],
   "metadata": {
    "id": "q24ASJ1Tftg1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(device):\n",
    "  \"\"\"\n",
    "  The function predict() loads a pre-trained speech recognition model and evaluates its performance on a test dataset.\n",
    "  :param device: The device to load the model on.\n",
    "  \"\"\"\n",
    "  model = SpeechRecognitionModel(\n",
    "      hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "      hparams['n_class'],  hparams[\"n_feats\"], hparams['stride'], hparams['dropout']\n",
    "  ).to(device)\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  criterion = nn.CTCLoss(blank=27).to(device)\n",
    "  load_model_weights(model, MODEL_FILE_PATH)\n",
    "  test_loader = get_data_loader(dir_path + TEST_PATH,hparams, is_train=False)\n",
    "  test_wer, test_cer = test(model, device, test_loader, criterion, is_print=True)\n",
    "  print(\"test wer: \" + str(test_wer))\n",
    "  print(\"test cer: \" + str(test_cer))\n",
    "\n",
    "predict(device)\n"
   ],
   "metadata": {
    "id": "0eAx_cFkeY4N",
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
